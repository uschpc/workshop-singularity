<!DOCTYPE html>
<html>
  <head>
    <title>Software Containers with Singularity</title>
    <meta charset="utf-8">
    <meta name="author" content="Center for Advanced Research Computing at University of Southern California" />
    <link rel="stylesheet" href="main.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: left, top, title-slide

# Software Containers with Singularity

### Derek Strong <br> dstrong[at]usc.edu <br> <br> Center for Advanced Research Computing <br> University of Southern California <br>

### Last updated on 2021-10-20

---

## Outline

1. Overview of software containers  
2. Getting pre-built container images  
3. Building custom container images  
4. Running containers

---

## What are software containers?

- Portable, reproducible, stable, and secure software environments
- Bundles a main application and all its dependencies
- Provides a custom user space
- OS-level virtualization
- Different from a virtual machine
  - Containers have direct access to the kernel
  - Virtual machines have indirect access (performance penalty)

---

## Singularity containers

- A container format designed for shared HPC systems
- Most popular container format for HPC
- Can be isolated from or integrated with host system
- Singularity vs. Docker
  - Docker requires superuser privileges to run container images
  - Superuser privileges not available on shared HPC systems
  - Can convert from Docker to Singularity format
  - Or vice versa: [singularity2docker.sh](https://github.com/singularityhub/singularity2docker)

---

## Singularity architecture

<img src="singularity-architecture.png" height="450" />

Source: [Singularity Community and SingularityPRO on high-performance servers](https://sylabs.io/assets/white-papers/Sylabs_Whitepaper_High_performance_server_v3.pdf)

---

## Why use Singularity for research?

- Install anything you want (based on any Linux OS)
- Ease installation issues by using pre-built container images
- Ensure reproducible software environments
- Ensure the same software stack is used among a research group
- Ensure the same software stack is used across Linux systems (e.g., any HPC center)
- Run the same workflows across Linux systems by embedding runscripts in container images

---

## Some specific Singularity use cases

- Converting Docker images to Singularity images
- Using applications that need older or cutting-edge or very specific versions of software
- Bundling complex software stacks for easier distribution
- Using proprietary software (typically distributed as binaries) that depends on other software not available on host system

---

## Some limitations of Singularity

- Built for Linux systems
- Portability depends on a few factors
  - CPU architecture format (x86 vs. ARM)
  - Binary format (ELF)
  - Kernel, glibc, other API compatibility
- Not always backwards compatible
- Need superuser privileges to build images

---

## The container image

- A single, compressed, read-only image file (executable `.sif`)
- Bundles application and all software dependencies needed to run it
- Intended to be immutable
- If need to modify, rebuild
- A container is a running instance of a container image

---

## Getting and running images

- Getting images
  - Pull pre-existing images from container registries
  - Download images from software websites
  - Build your own custom image
- Running images
  - Run images in interactive or batch modes
  - Various options can be used to isolate/integrate with host system

---

## Pulling existing container images

- Pull from container registries
- [Singularity Cloud Library](https://cloud.sylabs.io/library)

```bash
singularity pull library://ubuntu:latest
```

- [Docker Hub](https://hub.docker.com/)

```bash
singularity pull docker://clearlinux:latest
```

---

## Building custom images externally

- Need to build outside of CARC systems (requires superuser privileges)
- But need a Linux OS
- Can build in a sandbox mode or with a definition file (recipe)
- Best approach is to use the cloud-based [Singularity Remote Builder](https://cloud.sylabs.io/home)
- Or use a virtual machine on your local computer (e.g., [Multipass](https://multipass.run/), [Virtual Box](https://www.virtualbox.org/))
- Alternatively, create a Docker container image locally, upload to Docker Hub, and then pull

---

## Building workflow

1. Create a definition file
2. Build image externally using the definition file
3. If error occurs, modify definition file and rebuild (back to step 1)
4. Transfer image file to CARC systems
5. Test image
6. If error occurs, modify definition file and rebuild (back to step 1)

---

## Definition files

- Recipe for building a container image
  - Start with a base Linux OS (e.g., Debian, Ubuntu, CentOS, Clear Linux)
  - Or start with existing container image from a registry
  - Then install software, add files, etc.
- Similar to a Dockerfile, but different syntax
- [CARC Singularity template definition files](https://github.com/uschpc/singularities)

---

## Structure of a definition file

```bash
# Header (required)
Bootstrap: ...
From: ...
# Sections (optional)
%files
    Copy files to the container from host (/source /destination)
%post
    Install software and libraries, write configuration files, etc.
%test
    Run commands to validate build process
%environment
    Define environment variables that will be set at runtime
%startscript
    Run commands when singularity instance start command is used
%runscript
    Run commands when singularity run command is used    
%labels
    Add metadata labels (name-value pair)
%help
    Describe the container and its intended use
```

---

## Example definition file

```bash
Bootstrap: docker
From: julia:1.6.3

%post
    mkdir /opt/depot
    export JULIA_DEPOT_PATH=/opt/depot
    julia -e 'using Pkg; Pkg.add("StatsKit")'

%test
    julia --version

%environment
    export LC_ALL=C
    export JULIA_DEPOT_PATH=/opt/depot

%runscript
    julia

%help
    Debian 10 with Julia 1.6.3 and the JuliaStats packages.
```

---

## Using Singularity Remote Builder

- [Singularity Container Services](https://cloud.sylabs.io/home) by [Sylabs](https://www.sylabs.io)
- Free service with up to 11 GB of storage
- Log in with other account (GitHub, GitLab, Google, Microsoft)
- Set up access token on CARC systems
- Use web interface to build
- Or build remotely from the command line

```bash
singularity build --remote julia.sif julia.def
```

---

## Inspecting container images

```bash
singularity inspect --deffile julia.sif

singularity inspect --environment julia.sif

singularity inspect --runscript julia.sif
```

---

## Running containers

- A container process is like any other Linux process
- Just a different software environment
- Three commands:
  - `singularity shell` &mdash; for an interactive shell within the container
  - `singularity exec` &mdash; for executing commands within the container
  - `singularity run` &mdash; for running a pre-defined runscript within the container

---

## Examples

```bash
singularity shell julia.sif

singularity exec julia.sif julia -e 'println("Hello world")'

singularity run julia.sif
```

---

## Filesystem within containers

- Unique filesystem within a container
  - `/bin`
  - `/lib`
  - `/usr`
- Some directories from the host system are automatically mounted to the container
  - `/dev`
  - `/home1/<username>` (`$HOME`)
  - `/proc`
  - `/sys`
  - `/tmp` (`$TMPDIR`)

---

## Bind mounting directories to containers

- Use the `--bind` option to mount files or directories
- For example, to add your current working directory and `/scratch` directory:

```bash
singularity exec --bind $PWD,/scratch/<username> julia.sif julia script.jl
```

- Can also change name when binding if needed:

```bash
singularity exec --bind $PWD:/mydir julia.sif julia script.jl
```

---

## Other useful options

- Often a good idea to use `--cleanenv` (or shorter `-e`)
- May need to use `--no-home` to exclude `/home1` directory
  - e.g., for Python, R, Julia containers
  - Packages are installed in `/home1` by default
  - Can lead to conflicts with software installed in container

```bash
singularity exec --no-home --bind $PWD julia.sif julia script.jl
```

---

## Running containers on GPUs

- Containers need to access host GPU driver
- Use `--nv` option to allow container access to driver
- Run `nvidia-smi` on GPU node to see current driver version and compatibility
- [GPU support docs](https://singularity.hpcng.org/user-docs/master/gpu.html)
- Example for a TensorFlow runscript:

```bash
singularity run --cleanenv --nv tf.sif
```

---

## Running containers with MPI

- Two approaches: hybrid vs. mount
- Pros and cons for each approach
- Less portable because it depends on MPI versions available on host system
- [MPI support docs](https://singularity.hpcng.org/user-docs/master/mpi.html)
- Example for an OpenMPI program:

```bash
srun --mpi=pmix_v2 -n $SLURM_NTASKS singularity exec openmpi.sif mpi_program
```

---

## Singularity environment variables

- Some useful environment variables can be set
- Add to `~/.bashrc` to automatically set every time you log in
- For example, change cache directory from `/home1` to `/scratch`:

```bash
export SINGULARITY_CACHEDIR=/scratch/<username>/.singularity
```

- For example, add certain bind paths to every container:

```bash
export SINGULARITY_BIND=/scratch/<username>,/project/<project_id>
```

---

## Example Slurm job script

```bash
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16GB
#SBATCH --time=1:00:00
#SBATCH --account=<project_id>

module purge

singularity exec --no-home --bind $PWD julia.sif julia script.jl
```

---

## Singularity documentation

- [Official docs](https://singularity.hpcng.org/user-docs/master/)  

```bash
singularity help

singularity help shell

singularity help exec

singularity help run
```

---

## Additional resources

- [Using Singularity on CARC Systems](https://carc.usc.edu/user-information/user-guides/software-and-programming/singularity)
- [CARC Singularity template definition files](https://github.com/uschpc/singularities)
- [Singularity website](https://singularity.hpcng.org/)
- [Singularity documentation](https://singularity.hpcng.org/user-docs/master/)
- [Singularity tutorial](https://singularity-tutorial.github.io/)
- [Singularity Remote Builder](https://cloud.sylabs.io/home)
- [Singularity Cloud Library](https://cloud.sylabs.io/library)
- [Docker Hub](https://hub.docker.com/)
- [BioContainers](https://biocontainers.pro)
- [Dockstore](https://www.dockstore.org/)
- [NVIDIA GPU Cloud Catalog](https://ngc.nvidia.com/catalog)

---

## CARC support

- [Submit a support ticket](https://carc.usc.edu/user-information/ticket-submission)
- [User Forum](https://hpc-discourse.usc.edu/)
- Office Hours
  - Every Tuesday 2:30-5pm
  - Register [here](https://carc.usc.edu/news-and-events/events)

---

## Exercises

- Pull a container image from a registry
- Build a custom container image
    - Create a definition file
    - Use the Remote Builder to build the custom image
    - Transfer the image to CARC systems
- Inspect a container image
- Bind mount a `/project` directory to a container and list its files
- Run a script or program through a container

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create({
        ratio: "16:10",
        highlightStyle: "ascetic",
      });
    </script>
  </body>
</html>
